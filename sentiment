import sqlite3
from mastodon import Mastodon
import json
import re
from datetime import datetime
from bs4 import BeautifulSoup
from selenium import webdriver
import time
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.dates as mdates

# Part 1: Connect to Mastodon API and collect data

mastodon = Mastodon(
    access_token="gT9H9Q6JexEx0FtGgt53tWBQR60Z3RqvZmQAYf2qJm4",
    api_base_url="https://mastodon.social"
)

with open('terms.json', 'r') as file:
    data = json.load(file)
    search_terms = data['terms']

conn = sqlite3.connect("sentiment.db")
cursor = conn.cursor()

cursor.execute('''
CREATE TABLE IF NOT EXISTS data (
               SID INTEGER PRIMARY KEY AUTOINCREMENT,
               Product TEXT,
               User TEXT,
               Date TEXT,
               Message TEXT,
               Sentiment TEXT
)
''')
conn.commit()

def clean_message(message):
    soup = BeautifulSoup(message, 'html.parser')
    text = soup.get_text()
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

for term in search_terms:
    print(f"Searching for {term}")
    results = mastodon.timeline_hashtag(term, limit=50)

    if not results:
        print(f"No results found for {term}")
        continue

    for status in results:
        user = status['account']['username']
        date = status["created_at"]
        raw_message = status['content']
        message = clean_message(raw_message)

        if not message:
            print("Empty message, skipping...")
            continue

        cursor.execute('''
        INSERT INTO data (Product, User, Date, Message, Sentiment)
        VALUES (?, ?, ?, ?, ?)
        ''', (term, user, date, message, None))
conn.commit()


# Part 2: Scrape Amazon Reviews

product_data = []
with open('terms.txt', 'r') as file:
    for line in file:
        line = line.strip()
        if not line or ',' not in line:
            print(f"Skipping invalid line: {line}")
            continue
        try:
            product, url = line.split(',', 1)
            product_data.append((product.strip(), url.strip()))
        except ValueError as e:
            print(f"Error processing line: {line}. Error: {e}")
            continue

driver = webdriver.Chrome()
driver.get('https://www.amazon.com/SAMSUNG-Display-Watchfaces-Exercise-International/product-reviews/B0CW3VWC3X/ref=cm_cr_getr_d_paging_btm_prev_1?ie=UTF8&reviewerType=all_reviews&pageNumber=1')
time.sleep(20)

conn = sqlite3.connect('amazon_reviews.db')
cursor = conn.cursor()

cursor.execute('''
    CREATE TABLE IF NOT EXISTS reviews (
        SID INTEGER PRIMARY KEY AUTOINCREMENT,
        product TEXT,
        user TEXT,
        date TEXT,
        message TEXT,
        sentiment TEXT DEFAULT ''
    )
''')

def extract_date(raw_date):
    if "on" in raw_date:
        date_part = raw_date.split("on")[1].strip()
    else:
        date_part = raw_date.strip()
    return date_part

for product_name, url in product_data:
    print(f"Scraping reviews for: {product_name}")

    while url is not None:
        driver.get(url)
        time.sleep(3)

        html_data = BeautifulSoup(driver.page_source, 'html.parser')
        reviews = html_data.find_all('li', {'data-hook': 'review'})

        for review in reviews:
            user = review.find('span', {'class': 'a-profile-name'}).text.strip()
            raw_date = review.find('span', {'data-hook': 'review-date'}).text.strip()
            message = review.find('span', {'data-hook': 'review-body'}).text.strip()

            date = extract_date(raw_date)

            cursor.execute('''
                INSERT INTO reviews (product, user, date, message) VALUES (?, ?, ?, ?)
            ''', (product_name, user, date, message))

        url_check = html_data.find('li', {'class': 'a-last'})
        if url_check and 'a-disabled' not in url_check.get('class', []):
            url = 'https://www.amazon.com' + url_check.a['href']
        else:
            url = None

    print(f"Finished scraping reviews for: {product_name}")

conn.commit()
conn.close()
driver.quit()


# Part 3: Sentiment Analysis for both Amazon and Mastodon data

def perform_sentiment_analysis(db_name, table_name, message_column, sentiment_column):
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()

    cursor.execute(f"SELECT SID, {message_column} FROM {table_name}")
    rows = cursor.fetchall()

    analyzer = SentimentIntensityAnalyzer()

    for sid, message in rows:
        sentiment_scores = analyzer.polarity_scores(message)
        compound_score = sentiment_scores['compound']
        pos_score = sentiment_scores['pos']
        neg_score = sentiment_scores['neg']
        neu_score = sentiment_scores['neu']

        if compound_score > 0.7 and pos_score > 0.3:
            sentiment = 'Strong Positive'
        elif compound_score > 0.2 and pos_score > neg_score:
            sentiment = 'Positive'
        elif -0.2 <= compound_score <= 0.2 and neu_score >= 0.6:
            sentiment = 'Neutral'
        elif -0.5 < compound_score < -0.2 and neg_score > pos_score:
            sentiment = 'Negative'
        else:
            sentiment = 'Strong Negative'

        cursor.execute(f"UPDATE {table_name} SET {sentiment_column} = ? WHERE SID = ?", (sentiment, sid))

    conn.commit()
    conn.close()


# Update sentiment columns for both Mastodon and Amazon databases
perform_sentiment_analysis('sentiment.db', 'data', 'Message', 'Sentiment')
perform_sentiment_analysis('amazon_reviews.db', 'reviews', 'message', 'sentiment')


# Part 4: Data Visualization for sentiment trends and distributions

def plot_sentiment_data(db_name, table_name):
    conn = sqlite3.connect(db_name)
    query = f"""
    SELECT SID, product, dateconverted AS date, sentiment
    FROM {table_name}
    WHERE dateconverted IS NOT NULL AND sentiment != ''
    """
    df = pd.read_sql_query(query, conn)
    conn.close()

    df['date'] = pd.to_datetime(df['date'])

    sentiment_map = {'Strong Positive': 2, 'Positive': 1, 'Neutral': 0, 'Negative': -1, 'Strong Negative': -2}
    df['SentimentScore'] = df['sentiment'].map(sentiment_map)

    grouped_trend = df.groupby(['product', 'date'])['SentimentScore'].mean().reset_index()
    distribution = df.groupby(['product', 'sentiment']).size().reset_index(name='Counts')
    total_counts = distribution.groupby('product')['Counts'].sum().reset_index(name='Total')
    distribution = distribution.merge(total_counts, on='product')
    distribution['Percentage'] = (distribution['Counts'] / distribution['Total']) * 100

    sns.set(style="whitegrid", palette="muted", font_scale=1.2)

    unique_products = grouped_trend['product'].unique()
    num_products = len(unique_products)
    cols = 2
    rows = (num_products * 2 + cols - 1) // cols

    fig, axes = plt.subplots(rows, cols, figsize=(16, rows * 5))
    axes = axes.flatten()

    for i, product in enumerate(unique_products):
        product_data = grouped_trend[grouped_trend['product'] == product]
        ax = axes[i * 2]
        sns.lineplot(data=product_data, x='date', y='SentimentScore', ax=ax)
        ax.set_title(f"Sentiment Trend for {product}")
        ax.set_xlabel("Date")
        ax.set_ylabel("Average Sentiment Score")
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
        ax.xaxis.set_major_locator(mdates.MonthLocator())
        ax.tick_params(axis='x', rotation=45)

        product_distribution = distribution[distribution['product'] == product]
        pastel_colors = sns.color_palette("pastel")
        pie_colors = pastel_colors[:len(product_distribution['sentiment'])]
        ax = axes[i * 2 + 1]
        wedges, texts, autotexts = ax.pie(
            product_distribution['Percentage'],
            labels=None,
            autopct='%1.1f%%',
            startangle=140,
            colors=pie_colors,
            shadow=True
        )

        for autotext in autotexts:
            autotext.set_fontsize(8)

        ax.set_title(f"Sentiment Distribution for {product}")
        ax.legend(
            labels=product_distribution['sentiment'],
            title="Sentiments",
            loc="center left",
            bbox_to_anchor=(1, 0.5)
        )

    plt.tight_layout()
    plt.show()


# Plot sentiment data for both Mastodon and Amazon
plot_sentiment_data('sentiment.db', 'data')
plot_sentiment_data('amazon_reviews.db', 'reviews')
